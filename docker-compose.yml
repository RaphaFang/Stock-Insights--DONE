services:
  # app:
  # build:
  #   context: .
  #   dockerfile: Dockerfile.app
  # ports:
  #   - "8000:8000"

  app:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./:/app
    command: ["spark-submit", "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1", "/app/sp/spark_application_first.py"]
    env_file:
      - .env
    ports:
      - "8000:8000"
    depends_on:
      - kafka
      - zookeeper

  zookeeper:
    image: wurstmeister/zookeeper:latest
    cpus: 0.25
    mem_limit: 512m
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka:latest
    cpus: 0.75
    mem_limit: 1024m
    environment:
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    ports:
      - "9092:9092"

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
    cpus: 0.375
    ports:
      - "7077:7077"
      - "8080:8080"

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1 # 這邊是對內宣稱，worker可以用滿分配的0.75
      - SPARK_WORKER_MEMORY=1G
    cpus: 0.75
    depends_on:
      - spark-master
    ports:
      - "8081:8081"

  spark-driver:
    build:
      context: .
      dockerfile: Dockerfile
    tmpfs:
      - /opt/bitnami/spark/tmp
    container_name: spark-driver
    environment:
      - SPARK_MODE=client
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./:/app
      # - ./sp:/app/sp
      # - ./:/app # 把整個專案掛載到容器內的 /app 目錄
      # 但是這個會把我的另外連接性檔案覆蓋掉
    cpus: 0.375
    depends_on:
      - spark-master
    command: [
        "/opt/bitnami/spark/bin/spark-submit",
        "--master",
        "spark://spark-master:7077",
        "--deploy-mode",
        "client",
        "--packages",
        "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1",

        # cores, instances 的設置，core是說用到幾核心，instances是說用到幾台
        "--conf",
        "spark.driver.extraClassPath=/opt/bitnami/spark/custom-jars/*",
        "--conf",
        "spark.executor.cores=1",
        "--conf",
        "spark.executor.instances=1",
        "--conf",
        "spark.executor.memory=1g",
        "/app/sp/spark_application_first.py",
      ]
# ------------------------------------------------------------------------------
# version: "3.8"
# services:
#   zookeeper:
#     image: wurstmeister/zookeeper:latest
#     ports:
#       - "2181:2181"

#   kafka:
#     image: wurstmeister/kafka:latest
#     ports:
#       - "9092:9092"
#     environment:
#       KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
#       KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
#       KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#     volumes:
#       - /var/run/docker.sock:/var/run/docker.sock

#   app:
#     build: .
#     env_file:
#       - .env
#     ports:
#       - "8000:8000"
#     depends_on:
#       - kafka
#       - zookeeper

